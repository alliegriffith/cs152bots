# Install transformers from source - only needed for versions <= v4.34
# pip install git+https://github.com/huggingface/transformers.git
# pip install accelerate

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup
import torch.nn as nn
import pandas as pd
import wandb

# Vars
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

NUM_STEPS = 1000
WARM_UP = 150
LEARNING_RATE = 1e-5
BATCH_SIZE = 1

# Set up wandb
wandb.init(
    project="bot",
    name=f"train2",
    config={
        "base_model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "learning_rate": LEARNING_RATE,
        "batch_size": BATCH_SIZE,
        "num_steps": NUM_STEPS,
        "loss": "CEWithLogitsLoss"
    })

class Critic(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        
        # critic nn - will learn to predict probability of sextortion
        self.critic = nn.Sequential(
            nn.Linear(hidden_size, 2048), 
            nn.ReLU(),
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, 1)
        )
        
    def forward(self, input_ids): # input hidden state representation of the prompt + text
        
        scores = self.critic(input_ids)

        return scores

# load training and validation data in as pandas dataframes
train_df = pd.read_csv("/home/allie11/baselines/data/sextortion_train.csv")

# Verify by printing the first few rows
  
# # # initialize base model -> will take in the instructions + message -> we take the final transformer rep and feed to nn
model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0",output_hidden_states=True)
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0", padding_side = "left")
hidden_size = model.config.hidden_size

# init critic
critic = Critic(hidden_size)

model.to(device)
critic.to(device)

optimizer = torch.optim.AdamW(critic.parameters(), lr=LEARNING_RATE)

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=WARM_UP,
    num_training_steps=NUM_STEPS,
)

criterion = nn.BCEWithLogitsLoss()
# instructions = "Read the following text and output a value (0-1) representing how confident you are that the text was generated by a perpetrator of sextortion."


# loop through training data 
for i in range(len(train_df)):
        
    print("step:" )
    message = train_df.loc[i]["text"]
    label = train_df.loc[i]["label"]
    
    if label== "sextortion":
        target = torch.tensor([1.0]).to(device)
        
    else:
        target = torch.tensor([0.0]).to(device)
        
    
    # tokenize message
    message_Toks = tokenizer(message, return_tensors="pt").to(device)
    #print("message toks:", message_Toks)
    
    # run message through tinyllama to get transformer rep
    outputs = model(input_ids=message_Toks["input_ids"].to(device), output_hidden_states=True, return_dict_in_generate=True, do_dropout=False)
    
    hidden_last_token = outputs.hidden_states[-1][:, -1, :].to(device)
    
    score = critic(hidden_last_token).view(-1)   # shape [batch] 
    print("critic's score", score)
    loss = criterion(score, target).to(device)
    print("loss:", loss)
            
    # backprop loss through the critic network
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    scheduler.step()
    
    wandb.log({
            "loss": loss.item(),
            "step": i,
        })
# once exit training loop, save model
critic_save_path = "critic_sextortion.pt"
torch.save(critic.state_dict(), critic_save_path)
print(f"Saved critic base‚Äêmodel weights to {critic_save_path}")   
    # save critic - HELP
    
    
            
        
